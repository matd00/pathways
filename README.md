# pathways (Python Version)


pathways is a **fast multithreaded tool** for exploring and tracing pathways within any website. This project is inspired by the original [pathways in Rust](https://github.com/YuriRDev/pathways), rewritten in Python for learning and experimentation.


## ğŸ§° Getting Started


### â€¼ï¸ Prerequisites
Make sure you have Python 3.8+ installed. Then install dependencies:


```bash
pip install -r requirements.txt
```


### ğŸƒ Run Locally
Clone the project:


```bash
git clone https://github.com/matd00/pathways.git
```


Go to the project directory:


```bash
cd pathways/src
```


Run pathways:


```bash
python main.py --url "https://yourwebsite.com"
```


## ğŸ‘€ Usage


### Table of Args
| Name  | Type | Default | Description |
|---------------|----------|---------|-------------|
| `--url` | String | - | Initial URL to start scanning. |
| `--max-urls` | Integer | 1000 | Maximum number of URLs to discover. |
| `--match-str` | String | "" | Only keep URLs containing this string. |
| `--ignore` | List | [] | Ignore URLs containing any of these substrings. |
| `--concurrency` | Integer | 10 | Number of concurrent threads. |
| `--depth` | Integer | 3 | Maximum crawl depth. |


### Example


```bash
python main.py --url "https://yourwebsite.com" \
--match-str "yourwebsite.com" \
--ignore "wordpress" --ignore "wp" \
--concurrency 10 --depth 3
```


## ğŸ§­ Roadmap
- [ ] Prevent loops (donâ€™t re-add visited URLs).
- [ ] Improve queue handling with priority queue.
- [ ] Add `--depth` argument (done âœ…).
- [ ] Add export formats (CSV, SQLite).
- [ ] Add async mode with `asyncio` and `aiohttp`.


## ğŸ“‚ Project Structure
```
src/
â”œâ”€â”€ interface/
â”‚ â””â”€â”€ database.py # Storage and saving results
â”œâ”€â”€ lib.py # Core crawling logic
â”œâ”€â”€ main.py # CLI + Orchestration
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # Documentation
```


## ğŸ“œ License
MIT License. See `LICENSE` file for details.